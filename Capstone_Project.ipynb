{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing as prep\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix as cm\n",
    "from tkinter import *\n",
    "from tkinter import filedialog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and the dataset\n",
    "file_name=filedialog.askopenfilename()\n",
    "data=pd.read_csv(file_name,skiprows=0,encoding='utf-8')\n",
    "\n",
    "# Using MinMax to precroess the data\n",
    "df=data.iloc[:,:-1]\n",
    "df=(df-df.min(axis=0))/(df.max(axis=0)-df.min(axis=0))\n",
    "data.iloc[:,:-1]=df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shows the features of this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The features of this dataset are: \n",
      "['Elevation', 'Aspect', 'Slope', 'Horizontal_Distance_To_Hydrology', 'Vertical_Distance_To_Hydrology', 'Horizontal_Distance_To_Roadways', 'Hillshade_9am', 'Hillshade_Noon', 'Hillshade_3pm', 'Horizontal_Distance_To_Fire_Points', 'Wilderness_Area1', 'Wilderness_Area2', 'Wilderness_Area3', 'Wilderness_Area4', 'Soil_Type1', 'Soil_Type2', 'Soil_Type3', 'Soil_Type4', 'Soil_Type5', 'Soil_Type6', 'Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type10', 'Soil_Type11', 'Soil_Type12', 'Soil_Type13', 'Soil_Type14', 'Soil_Type15', 'Soil_Type16', 'Soil_Type17', 'Soil_Type18', 'Soil_Type19', 'Soil_Type20', 'Soil_Type21', 'Soil_Type22', 'Soil_Type23', 'Soil_Type24', 'Soil_Type25', 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type29', 'Soil_Type30', 'Soil_Type31', 'Soil_Type32', 'Soil_Type33', 'Soil_Type34', 'Soil_Type35', 'Soil_Type36', 'Soil_Type37', 'Soil_Type38', 'Soil_Type39', 'Soil_Type40']\n",
      "\n",
      "The number of different types of forrest is: 7\n",
      "\n",
      "The number of samples in each category is: [211840, 283301, 35754, 2747, 9493, 17367, 20510]\n"
     ]
    }
   ],
   "source": [
    "print('The features of this dataset are: ')\n",
    "print(data.iloc[:,:-1].columns.tolist())\n",
    "print()\n",
    "n_class=len(np.unique(data.iloc[:,-1].values))\n",
    "print('The number of different types of forrest is: '+str(n_class))\n",
    "print()\n",
    "# initialize an empty list to store info\n",
    "n_samples=[]\n",
    "for i in range(n_class):\n",
    "    df_sub=data[data['Cover_Type']==i+1]\n",
    "    n_sample=df_sub.shape[0]\n",
    "    n_samples.append(n_sample)\n",
    "    \n",
    "print('The number of samples in each category is: '+str(n_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset is highly skwed and thus we need to sample from each category to construct the new dataset for both training and testing purpose. We'll run the sampling policy 5 times and use the average score as the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sampling from original dataset is finished\n"
     ]
    }
   ],
   "source": [
    "d=[]\n",
    "# in each category, we sample 2000 samples\n",
    "num_samples=2000\n",
    "for i in range(n_class):\n",
    "    df_sub=data[data['Cover_Type']==i+1]\n",
    "    # sample this subset\n",
    "    df_sub=df_sub.sample(num_samples)\n",
    "    # reset the index\n",
    "    df_sub=df_sub.reset_index(drop=True,inplace=False)\n",
    "    # append it into list\n",
    "    d.append(df_sub)   \n",
    "\n",
    "# concat sublist together\n",
    "d_init=d[0]\n",
    "for i in range(1,n_class):\n",
    "    # update the initial dataset\n",
    "    d_init=pd.concat(objs=[d_init,d[i]],axis=0)\n",
    "    # reset index\n",
    "    d_init=d_init.reset_index(drop=True,inplace=False)\n",
    "    \n",
    "# update the dataset\n",
    "d=d_init\n",
    "print('The sampling from original dataset is finished')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next thing is to seperate the dataset into training and test set. 80% of training set and 20% of test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of training set input is: (11200, 54)\n",
      "The shape of training set outcome is: (11200, 7)\n",
      "The shape of test set input is: (2800, 54)\n",
      "The shape of test set outcome is: (2800, 7)\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into input X and outcome Y\n",
    "X=d.iloc[:,:-1]\n",
    "Y=d.iloc[:,-1]\n",
    "# apply one-hot encoding on Y\n",
    "y=Y.values\n",
    "Y=np.zeros((len(y),n_class))\n",
    "for i in range(Y.shape[0]):\n",
    "    ind=int(y[i])\n",
    "    Y[i,ind-1]=1\n",
    "    \n",
    "# apply the train test split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,shuffle=True)\n",
    "\n",
    "print('The shape of training set input is: '+str(X_train.shape))\n",
    "print('The shape of training set outcome is: '+str(Y_train.shape))\n",
    "print('The shape of test set input is: '+str(X_test.shape))\n",
    "print('The shape of test set outcome is: '+str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply classification algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we'll implement three different algorithms\n",
    "\n",
    "1. KNN\n",
    "2. SVM\n",
    "3. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply knn algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# try with 10 different values of k to find the best one\n",
    "Ks=10\n",
    "mean_acc=np.zeros((Ks-1))\n",
    "std_acc=np.zeros((Ks-1))\n",
    "CM=[];\n",
    "mean_acc=np.zeros((Ks,))\n",
    "for n in range(1,Ks+1):\n",
    "    #Train Model and Predict  \n",
    "    neigh=KNeighborsClassifier(n_neighbors=n).fit(X_train,Y_train)\n",
    "    y_hat=neigh.predict(X_test)\n",
    "    # get the accuracy rate\n",
    "    y_pred=np.argmax(y_hat,axis=1)\n",
    "    y_true=np.argmax(Y_test,axis=1)\n",
    "    acc=np.sum((y_true==y_pred)*1)/Y_test.shape[0]\n",
    "    mean_acc[n-1]=acc\n",
    "    # get the confusion matrix\n",
    "    d=cm(y_true,y_pred)\n",
    "    CM.append(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best number of neighbor is: 1\n",
      "And the correspond accuracy rate is: 0.8167857142857143\n",
      "\n",
      "The best confusion matrxi is: \n",
      "[[293  74   0   0  11   3  13]\n",
      " [ 87 260  11   0  39  10   0]\n",
      " [  0   6 291  35   7  78   0]\n",
      " [  0   0  19 413   0  14   0]\n",
      " [  5  19   4   0 355   2   0]\n",
      " [  0   3  42   4   5 315   0]\n",
      " [ 17   5   0   0   0   0 360]]\n"
     ]
    }
   ],
   "source": [
    "# test result \n",
    "print('The best number of neighbor is: '+str(np.argmax(mean_acc)+1))\n",
    "print('And the correspond accuracy rate is: '+str(mean_acc[np.argmax(mean_acc)]))\n",
    "print()\n",
    "print('The best confusion matrxi is: ')\n",
    "print(CM[np.argmax(mean_acc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply SVM algorithm\n",
    "from sklearn import svm\n",
    "clf=svm.LinearSVC(penalty='l1',dual=False)\n",
    "clf.fit(X_train,np.argmax(Y_train,axis=1))\n",
    "y_hat=clf.predict(X_test)\n",
    "\n",
    "# get the accuracy rate\n",
    "y_pred=y_hat\n",
    "y_true=np.argmax(Y_test,axis=1)\n",
    "acc=np.sum((y_true==y_pred)*1)/Y_test.shape[0]\n",
    "# get the confusion matrix\n",
    "d=cm(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: 0.6778571428571428\n",
      "\n",
      "The confusion matrix is: \n",
      "[[248  64   0   0  34   4  44]\n",
      " [ 73 213  10   0  98  10   3]\n",
      " [  0   2 206  57  18 134   0]\n",
      " [  0   0  27 393   0  26   0]\n",
      " [ 12  55  27   0 274  17   0]\n",
      " [  0  15  60  26  35 233   0]\n",
      " [ 46   2   1   0   2   0 331]]\n"
     ]
    }
   ],
   "source": [
    "# test result\n",
    "print('The test accuracy is: '+str(acc))\n",
    "print()\n",
    "print('The confusion matrix is: ')\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LogR=LogisticRegression(penalty='l2',C=0.01,solver='lbfgs',multi_class='multinomial').fit(X_train,np.argmax(Y_train,axis=1))\n",
    "y_hat=LogR.predict(X_test)\n",
    "\n",
    "# get the accuracy rate\n",
    "y_pred=y_hat\n",
    "y_true=np.argmax(Y_test,axis=1)\n",
    "acc=np.sum((y_true==y_pred)*1)/Y_test.shape[0]\n",
    "# get the confusion matrix\n",
    "d=cm(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The test accuracy is: 0.6075\n",
      "\n",
      "The confusion matrix is: \n",
      "[[174 142   0   0  36   2  40]\n",
      " [ 53 248   6   2  84  10   4]\n",
      " [  0   2 153  89  28 145   0]\n",
      " [  0   0  24 381   0  41   0]\n",
      " [ 32  78  30   0 234  11   0]\n",
      " [  2  15  45  61  51 195   0]\n",
      " [ 32  31   1   0   2   0 316]]\n"
     ]
    }
   ],
   "source": [
    "# test result\n",
    "print('The test accuracy is: '+str(acc))\n",
    "print()\n",
    "print('The confusion matrix is: ')\n",
    "print(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
